\section{Problem Definition}

\subsection{Limitations of Manual Software Testing}

A crucial aspect of software development is ensuring that the written code behaves as intended.  Today, this is particularly important, as software is playing an increasingly dominant role in our lives, from keeping our private data in the cloud to powering our ubiquitous smartphones.

Ideally, software would be written correctly by construction.  This was the way software engineering was envisioned in the early days of computer science~\cite{dijkstra1976discipline}; the code would be written together with the mathematical proof of its correctness.

However, the decades of software development experience that followed have painted a different picture.  As it turns out, the software complexity---up to hundreds of millions of lines of code---and rapid pace of feature development have precluded a formal, rigorous approach.
%
With the exception of a few safety-critical segments, such as avionics~\cite{Astree}, automotive~\cite{automotive}, or medical equipment, most of the software industry today relies on \emph{testing} for its quality assurance~\cite{softwareMetrics}.

Testing a program consists of exercising multiple different paths through it and checking whether ``they do the right thing.''
%
In other words, testing is a way to produce partial evidence of correctness, and thus increase confidence in the tested software.
%
Yet, test suites provide inadequate coverage of all the inputs a program could handle.
%% Yet, due to the typically poor coverage one can get today, testing often turns into a mere hunt for bugs.
%
For instance, the test suite of the Chromium browser contains hundreds of thousands of tests~\cite{chrome-tests}.  This suite is thoroughly comprehensive by industry standards, yet it represents only a small fraction of all possible inputs the browser may receive.

%% In practice, most software test harnesses consist of manually written tests that are run periodically; regression test suites provide an automated way of checking whether new bugs have entered the code~\cite{codeComplete}.
%
Test suites also tend to be tedious to write and maintain.  Statistics show that, on average, developers spend as much as half of their time doing testing~\cite{codeComplete}, and companies allocate up to a quarter of their IT budget for software testing~\cite{capgemini-world-quality}.

The net result is that, despite the effort and resources invested in manual testing, bugs escape quality assurance and make it into production~\cite{redHatSecurityX}.
%
The industry average for bug density is a staggering 15 errors per 1000 lines of shipped code, while the most thorough quality assurance processes reach 0.1 errors per 1000 lines of code~\cite{codeComplete}.

Today, this problem is amplified by having an increasing number of businesses online and hence vulnerable to remote attacks that exploit software vulnerabilities.  For instance, in 2014, there were 19 CVE vulnerabilities reported on average \emph{per day}~\cite{nvd}.
%
These vulnerabilities are exploited to cause major disruptions~\cite{sony-hack} or leaks of sensitive data~\cite{sony-hack,psp-hack}.  On average, a data breach cost \$3.8 million in 2015~\cite{breach2015}, and the most prominent cases cost over \$100 million~\cite{target-hack}.

%% Software testing is resource-hungry, time-consuming, labor-intensive, and prone to human omission and error.  A lot of bugs escape quality assurance into production, where they cause disruptions and data loss. For instance, in 2014, there were 19 CVE vulnerabilities reported on average per day, and estimates show that software bugs cost the global economy more than \$300 billion in 2013.

%% At the same time, resources invested in testing are peaking. In 2014, about a quarter of the entire IT budget in enterprises was allocated to software testing, which was the highest figure to that date.  On average, developer teams spend as much as half of their time doing testing and fixing bugs.

%% Facing this situation, organizations recognize that software testing is cumbersome.  In a recent survey, more than two thirds of the organizations questioned admitted they have difficulties determining relevant tests for their software and pointed at the lack of adequate tools to support that.

Alas, with a few exceptions---seL4~\cite{seL4}, a recent effort of formally verifying an operating system kernel---switching to a formal development model is not feasible for most of the software written today.
%
Despite recent advancements in formal techniques and tools, which have brought down the cost of building formally-proven software, it still takes on the order of person-years to develop a few thousand lines of verified code~\cite{seL4}.
%
This rate is currently unsustainable for most commodity software.
%
Therefore, the second best option today is to automate the software testing process itself.


\subsection{Automated Testing: Black-box vs. White-box}

The simplest (and most popular) form of automated testing consists of randomly generating program inputs and observing whether the program crashes~\cite{fuzz,quickcheck,afl,autodafe,skipfish}.
%
The inputs are typically obtained by fuzzing~\cite{fuzz}, i.e., randomly mutating a known valid input, such as an image file or a productivity suite document.  This form of testing is called ``blackbox'', because the input generation does not take into account the structure of the program under test~\cite{blackbox-testing}.
%
Despite their conceptual simplicity, fuzzers are effective at discovering  bugs~\cite{afl,autodafe,skipfish} (albeit shallow) and are currently the state of the practice in automated testing.

\begin{figure}
  \centering
  \includegraphics[width=2.0in]{figures/intro/fuzzing-example}
  \caption{Example program for which random fuzzing is unlikely to uncover the memory error bug (the red line).}
  \label{fig:intro:fuzzing}
\end{figure}

However, plain random fuzzers are ineffective at discovering corner-case bugs---bugs that manifest only under particular inputs in the program.
%
Consider the simple example in Figure~\ref{fig:intro:fuzzing}, where the program checks the input for the particular value 42 before performing a null-pointer access.  A random fuzzer has \emph{less than one in a billion} chance of hitting the bug with each input generated.
%
The vast majority of the generated inputs are therefore redundant.

A better approach is ``greybox'' testing, i.e., to generate tests that follow a certain structure, as defined by a specification, such as protocol format~\cite{sulley}, grammar~\cite{quickcheck}, or input precondition~\cite{boyapati:korat}.
%
However, this method would still likely miss bugs whose triggering inputs are not exposed in the input specification.

The most precise approach is to take the program structure itself into account when generating inputs.
%
This form of testing is called ``whitebox''; its goal is to generate inputs that take the program along previously unexplored execution paths.
%
Symbolic execution is the most successful automated whitebox testing technique to be applied to commodity software, in terms of size of software and bugs found~\cite{sage2012,all-symbex,decades-symbex,practice-symbex}.

\subsection{Systematic Path Exploration Using Symbolic Execution}
\label{sec:intro:symbex}

\begin{figure}
  \centering
  \includegraphics[width=3.5in]{figures/intro/symbex-tree}
  \caption{Symbolic execution tree for a simple example.  Program states maintain symbolic values for variable \codebit{x} and carry a path condition (pc).}
  \label{fig:intro:symbex-tree}
\end{figure}

Symbolic execution works by executing a program with \emph{symbolic} instead of concrete input values, to cover the entire input space and thus all possible program execution paths (Figure~\ref{fig:intro:symbex-tree}).  For instance, a function \codebit{foo(int x)} is executed with a symbolic variable $\lambda$ assigned to \codebit{x}.
%
Statements using \codebit{x} manipulate it symbolically: \codebit{x := x * 3} updates the value of \codebit{x} to $3 \cdot \lambda$.  During execution, the assignment of variables to expressions is kept in a \emph{symbolic store} of the program execution state.

Whenever the symbolic execution engine encounters a conditional branch, the execution state forks into two states, whose executions proceed independently.  For each state, this process repeats for subsequent branches, turning an otherwise linear execution into a \emph{symbolic execution tree}.

Each program state keeps the conjunction of the branch conditions taken as a \emph{path condition}.
%
The path condition is a formula over the symbolic program inputs, whose solutions are concrete input assignments (e.g., $\lambda = 42$) that take the program along the same execution path.
%
The satisfiability of the formula is decided by a constraint solver, which is typically an external off-the-shelf tool~\cite{stp,Z3,cvc}.
%
The symbolic execution engine queries the solver whenever it needs to decide the feasibility of each branch condition, or when it generates a \emph{test case} at the end of an execution path.

Symbolic execution is \emph{complete}, as it systematically enumerates all program paths.
%
It is also \emph{sound}, because each execution path corresponds to a real execution, replayable using a test case.
%
The two properties make symbolic execution highly effective at automatically generating high-coverage test suites~\cite{klee}, finding bugs~\cite{sage2012,mayhem}, and even debugging~\cite{esd,oasis,portend,king:symbolic:2}.

% The satisfiability problem is NP-complete, which means there is no efficient (polynomial) algorithm available.  In practice, modern solvers can handle most formulas efficiently by using heuristics.  However, symbolic execution time is still dominated by constraint solving, as formulas can quickly become complex for non-trivial software.

%% The number of paths in the symbolic execution tree is roughly exponential in the number of program branches, with loops making the problem even worse.  As any non-trivial software can have at least thousands of branches, the size of the tree quickly becomes too large to be exhaustively covered in a reasonable amount of time.  This is called the ``path explosion'' problem.

Alas, symbolic execution is challenging to apply to non-trivial software.  The number of paths in the symbolic execution tree is roughly exponential in the number of program branches, with loops further amplifying the problem.  Even for moderately sized programs with hundreds of branches, the size of the execution tree becomes too large to be completely covered in a reasonable amount of time.
%
This is commonly known as the ``path explosion'' problem and, in practice, symbolic execution engines resort to prioritizing paths within a given time budget, guided by a pluggable component in the engine called a \emph{search strategy}.


\subsection{The Environment Problem in Symbolic Execution}

%% However, its adoption in industrial general-purpose software testing has been hampered by the path explosion problem---the fact that the number of paths through a program is roughly exponential in program size.

For the software commonly encountered in the real world, with thousands and even millions of lines of code, mitigating path explosion alone is not sufficient, given the sheer size of the symbolic execution tree.
%
For instance, consider a small system utility that prints at the terminal the contents of a file.  To perform this task, the program calls the operating system for accessing files and printing on the screen.
%
Even if we're only interested in testing solely the code of the utility, executing it involves traversing tens of thousands of lines of kernel and library code.

Instead, the symbolic execution engine should focus the exploration on the system utility.  At the same time, the techniques used to reduce the time spent in its environment should still provide to the program consistent data from the environment, in order to keep the execution sound and avoid missing feasible paths through the program.
%
This is called the ``environment problem'' in symbolic execution and it is the main scalability issue of applying symbolic execution on real-world software.

One simple approach for dealing with the environment problem is to let the calls into the environment go through concretely.  For instance, the symbolic parameters of a system call could be forcefully assigned concrete values before executing the system call.
%
However, this approach risks introducing inconsistencies in the execution and miss feasible paths in the target program itself.
%
For example, if opening a file with a symbolic name would either succeed or fail depending on the file name, concretizing to a valid file name would cause the symbolic execution to miss the error handling path in the program.
%
For this example, a possible approach to simplifying the environment, while maintaining program coverage, is to inject the system call failure at the operating system interface, and hence exercise both successful and failing paths in the program.

\newcommand{\cnineccolor}{\cellcolor{GreenYellow}}
\newcommand{\chefccolor}{\cellcolor{SkyBlue}}

\begin{table}
  \centering
  \small
  \begin{tabular}{r p{5cm} p{5cm}}
    
    \textbf{Abstraction} & \cnineccolor Same-level & \chefccolor Nested \\
    \hline
    \bigskip Examples      & C program using a numerical library.    & Program written in a numerical computing language (e.g., Julia). \\
    
    \textbf{Encapsulation} & \chefccolor Loose & \cnineccolor Strong \\
    \hline
    \bigskip Examples      & Linux file system implemented as a kernel module.   & Linux file system implemented using the FUSE user-space interface. \\
    
    \textbf{Stability}   & \cnineccolor Stable    & \chefccolor Frequently-changing \\
    \hline
        Examples          & A web application that uses a standard web interface, such as CGI.    & A web application that uses the fast-changing native API of its web server (e.g., Apache). \\
  \end{tabular}
  \caption{Examples of environment interfaces, classified along main design principles---abstraction and encapsulation---and stability in time.  Color overlays indicate design space coverage by \colorbox{GreenYellow}{\cnine} and \colorbox{SkyBlue}{\chef}.}
  \label{tab:intro:env}
\end{table}

In general, systematically addressing the environment problem is challenging, as its manifestations depend on the nature of the environment interface and its implementation.

To roughly structure the problem space, we classify in Table~\ref{tab:intro:env} the instances of the environment problem along three main attributes of the environment interface: abstraction, encapsulation, and interface stability in time.
%
Abstraction and encapsulation are two essential principles in system design.  In particular, for program environments, abstraction refers to hiding the complexity of data representation and handling behind simpler concepts exposed to the program.  Encapsulation refers to restricting access to the environment state through narrow, well-defined interfaces.
%
Finally, interface stability refers to the degree of churn in the interface specification.  For instance, well-documented and standardized interfaces tend to change less often.

Table~\ref{tab:intro:env} illustrates with examples the different levels of abstraction, encapsulation, and stability occurring in real-world situations.  For each example, a symbolic execution engine would need to approach the environment in fundamentally different ways.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Solution Overview: A Tailored Approach to The Environment Problem}

This thesis addresses two instances of the environment problem, which collectively cover a wide range of real-world software: (1) system software interacting with the operating system, and (2) high-level programs written in dynamic languages, such as Python, Ruby, or JavaScript.
%
The two instances exhibit opposite characteristics.

On the one hand, the operating system interfaces are typically stable and well-documented (e.g., POSIX).  Moreover, while they consist of hundreds of system calls, their usage follow a power-law distribution, such that most software only uses a small fraction of them.

On the other hand, the specifications of dynamic languages are incomplete and unstable.
%
Moreover, these languages rely heavily on built-in functions, i.e., functions that are not implemented in the language itself, but are C code part of the interpreter.  This makes the program/environment interface highly ``porous'', as the built-in calls may have arbitrary side effects on the program state.

Both problem instances cover both levels of abstraction, encapsulation, and stability, as the color overlays show in Table~\ref{tab:intro:env}.

\subsection{Symbolic Models for a Stable Operating System Interface}

To address the environment problem for stable operating system interfaces, this thesis introduces \cnine, a symbolic execution platform with an accurate and efficient operating system model, as used by systems such as system utilities, web servers, or distributed systems.

\cnine relies on the insight that, for the purpose of testing, the operating system interface can be modeled as guest code on top of a set of basic abstractions: threads and processes, synchronization, and address spaces with shared memory.
%
These abstractions need to be provided as primitives by the symbolic execution engine, while the rest of the model can be emulated as guest code, by substituting the standard C library used by the target programs.

We prototyped \cnine for system code that uses the standard POSIX interface.  \cnine provides accurate and efficient models for files, network sockets, threads, processes, synchronization, IPC, signals, and other miscellaneous functions.
%
By using these models as substitutes for the complex kernel implementations, \cnine is the first to efficiently target complex system utilities such as \textsf{curl}, web servers such as Apache and lighttpd, or other networked services, such as memcached.
%
As a result, \cnine uncovered new bugs, including a security vulnerability in memcached, and generated high-coverage test suites.


\subsection{Using Interpreters as Specifications for Fast-changing Languages}

For programs written in high-level dynamic languages, such as Python, Ruby, or JavaScript, the environment problem lends itself to a different approach.
%
Building a symbolic execution engine by hand for these languages is a significant engineering undertaking.  The language semantics are complex, loosely specified, and change frequently.  Moreover, they rely heavily on built-in functionality that ought to be modeled in the symbolic execution engine.

This thesis introduces the idea of using the language interpreter---the de facto standard of the language semantics---as an ``executable language specification'': the interpreter runs in a lower-level (e.g., x86) symbolic execution engine, while it executes the target program.  In turn, the aggregate system acts as a high-level symbolic execution engine for the target program.

To map the low-level interpreter paths to high-level program paths, we partition the interpreter paths into chunks, one for each high-level instruction executed on the path. Then, the low-level paths with the same sequence of high-level instructions map to the same high-level path.

To circumvent the path explosion arising in the interpreter, we introduce Class-uniform Path Analysis (CUPA), a family of path prioritization heuristics for maximizing a given coverage metric.
%
CUPA works by grouping paths into equivalence classes, according to a coverage goal.  The prioritization is done by uniformly choosing groups instead of paths.  As a result, any path selection bias introduced by program locations with higher path explosion is contained within one equivalence class.

We prototyped these ideas in the \chef symbolic execution platform for interpreted languages.  With \chef, we obtained engines for Python and Lua, which generated test suites and found bugs in popular library packages.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Thesis Roadmap}

The rest of the thesis presents in detail the design, implementation, and evaluation of the two approaches to the environment problem.

Chapter~\ref{ch:relatedwork} provides more background on symbolic execution and the environment problem, by surveying the related work and positioning our contributions with respect to it.

Chapter~\ref{ch:cloud9} expands on the idea of modeling the operating system interface.  It presents the core abstractions that are built into the symbolic execution engine, then elaborates on the specifics of modeling the POSIX interface on top of them.

Chapter~\ref{ch:chef} presents the approach of using interpreters as executable language specifications.  It presents an overview of the Chef system, then it goes into the details of converting between the low-level symbolic execution of the interpreter and the high-level program execution.

Chapter~\ref{ch:evaluation} provides the experimental evaluation of the two systems we built, along two main dimensions: the effectiveness in generating test suites and finding bugs, and the scalability to real-world software.

Chapter~\ref{ch:paas} presents the ongoing work and longer-term vision of building a cloud-based testing service for cloud applications based on the techniques introduced in this thesis.

Chapter~\ref{ch:conclusion} ends the thesis with conclusions.

%%% Local Variables: 
%%% mode: latex
%%% eval: (visual-line-mode)
%%% fill-column: 1000000
%%% TeX-master: "main"
%%% End:
